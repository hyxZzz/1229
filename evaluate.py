import matplotlib
# å¼ºåˆ¶ä½¿ç”¨æ— å¤´æ¨¡å¼ï¼Œé˜²æ­¢æœåŠ¡å™¨æŠ¥é”™
matplotlib.use('Agg') 

import torch
import numpy as np
import yaml
import argparse
import matplotlib.pyplot as plt
from envs.combat_env import CombatEnv_8v8
from agents.red_rl.policy import RedPolicy
from utils.render_tool import RenderTool

def load_config(path):
    with open(path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def run_evaluation(model_path):
    # 1. åŠ è½½é…ç½®
    train_config = load_config("configs/train_config.yaml")
    
    # 2. åˆå§‹åŒ–ç¯å¢ƒ
    env = CombatEnv_8v8()
    obs_dict = env.reset()
    
    # 3. åˆå§‹åŒ–å¹¶åŠ è½½ç­–ç•¥
    policy = RedPolicy(train_config['train'])
    
    if model_path:
        print(f"Loading model from {model_path}...")
        state_dict = torch.load(model_path, map_location='cpu')
        policy.load_state_dict(state_dict)
    else:
        print("Warning: No model path provided, using random weights.")
    
    policy.eval() # åˆ‡æ¢åˆ°ç½‘ç»œè¯„ä¼°æ¨¡å¼ (LayerNormç­‰)
    
    # 4. åˆå§‹åŒ–æ¸²æŸ“å™¨
    renderer = RenderTool()
    
    # 5. ä»¿çœŸå¾ªç¯
    done = False
    step = 0
    max_steps = 4000 # å¯¹åº” 200ç§’
    
    red_uids = [f"Red_{i}" for i in range(8)]
    
    print(">>> Start Simulation Loop (Deterministic=False)...")
    
    total_fires = 0
    total_kills = 0

    try:
        while not done and step < max_steps:
            # è®°å½•ç”»é¢
            renderer.record_frame(env.sim)
            
            # --- æ„é€  Batch Obs ---
            batch_obs = {
                k: np.zeros((8, *v.shape), dtype=np.float32) 
                for k, v in list(obs_dict.values())[0].items()
            }
            
            for i, uid in enumerate(red_uids):
                if uid in obs_dict:
                    for k in batch_obs:
                        batch_obs[k][i] = obs_dict[uid][k]
            
            # --- ç­–ç•¥æ¨ç† ---
            # [å…³é”®ä¿®æ”¹] ä½¿ç”¨ Stochastic ç­–ç•¥ï¼Œè¿˜åŸè®­ç»ƒæ—¶çš„è¡Œä¸º
            acts, _, _ = policy.act(batch_obs, deterministic=False)
            
            # --- è½¬æ¢åŠ¨ä½œ ---
            env_actions = {}
            all_enemies = [p for p in env.sim.aircrafts if p.team == 1]
            
            for i, uid in enumerate(red_uids):
                if uid not in obs_dict: continue
                
                man_id = acts[0][i]
                tar_idx = acts[1][i]
                
                fire_target = None
                if tar_idx < len(all_enemies):
                    target_obj = all_enemies[tar_idx]
                    if target_obj.is_active:
                        fire_target = target_obj.uid
                    
                env_actions[uid] = {'maneuver': man_id, 'fire_target': fire_target}
                
            # --- ç¯å¢ƒæ­¥è¿› (å…³é”®ï¼šæ•è· events) ---
            # æˆ‘ä»¬éœ€è¦æ‰‹åŠ¨è°ƒç”¨ sim.step æ¥è·å– eventsï¼Œæˆ–è€…ä¿®æ”¹ combat_env è¿”å› events
            # è¿™é‡Œä¸ºäº†ä¸æ”¹åŠ¨ env ä»£ç ï¼Œæˆ‘ä»¬åˆ©ç”¨ combat_env å†…éƒ¨çš„é€»è¾‘
            # ä½†æ˜¯ combat_env.step å¹¶æ²¡æœ‰è¿”å› eventsï¼Œåªè¿”å›äº† rewards
            # æ‰€ä»¥æˆ‘ä»¬åªèƒ½é€šè¿‡å‰åçŠ¶æ€å¯¹æ¯”ï¼Œæˆ–è€…ä¿¡ä»» log
            
            # ä¸ºäº†è°ƒè¯•ï¼Œæˆ‘ä»¬åœ¨ combat_env.py å¤–é¢ç›´æ¥çœ‹ sim çš„å¯¼å¼¹å˜åŒ–æœ‰ç‚¹éš¾
            # æœ€ç®€å•çš„æ–¹æ³•ï¼šè§‚å¯Ÿ env.step åçš„ rewards
            # å¦‚æœ reward æœ‰å·¨å¤§çš„è·³å˜ (+10)ï¼Œè¯´æ˜æœ‰å‡»æ€
            # å¦‚æœ sim.missiles æ•°é‡å¢åŠ äº†ï¼Œè¯´æ˜æœ‰å¼€ç«
            
            prev_missile_count = len(env.sim.missiles)
            obs_dict, rewards, dones, info = env.step(env_actions)
            curr_missile_count = len(env.sim.missiles)
            
            # ç»Ÿè®¡å¼€ç«
            if curr_missile_count > prev_missile_count:
                new_fires = curr_missile_count - prev_missile_count
                total_fires += new_fires
                print(f"[Step {step}] ğŸ”¥ FIRE! Total fired: {new_fires}")

            # ç»Ÿè®¡å‡»æ€ (é€šè¿‡ Reward æ¨æ–­)
            for uid, r in rewards.items():
                if r > 5.0: # å‡»æ€å¥–åŠ±é€šå¸¸ > 5
                    print(f"[Step {step}] ğŸ’€ KILL CONFIRMED by {uid}! Reward: {r:.1f}")
                    total_kills += 1

            if dones.get("__all__", False):
                done = True
                
            step += 1
            
            # --- æ—¥å¿—æ‰“å° ---
            if step % 100 == 0 or done:
                red_alive = sum(1 for p in env.sim.aircrafts if p.team == 0 and p.is_active)
                blue_alive = sum(1 for p in env.sim.aircrafts if p.team == 1 and p.is_active)
                
                # è®¡ç®—æœ€è¿‘è·ç¦» (Min Dist) è€Œä¸æ˜¯ Mean Distï¼Œè¿™æ›´æœ‰æ„ä¹‰
                min_dist = 200000.0
                for r in env.sim.aircrafts[:8]:
                    if r.is_active:
                        for b in env.sim.aircrafts[8:]:
                            if b.is_active:
                                d = np.linalg.norm(r.pos - b.pos)
                                if d < min_dist: min_dist = d
                
                print(f"Step {step}: Red={red_alive} | Blue={blue_alive} | Min Dist={min_dist/1000:.1f}km | Fires={total_fires}")
                
                if blue_alive == 0:
                    print(">>> VICTORY! Blue Team Annihilated.")
                    break

    except KeyboardInterrupt:
        print("Interrupted.")
    
    print(f">>> Finished. Total Fires: {total_fires}, Total Kills: {total_kills}")
    
    print("Generating GIF...")
    if len(renderer.history) > 2000:
        renderer.history = renderer.history[:2000] # æˆªå–å‰2000å¸§
    renderer.animate(save_path="debug_result.gif")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", type=str, default="./checkpoints/model_epoch_100.pt")
    args = parser.parse_args()
    
    run_evaluation(args.model)