import numpy as np
import torch
import torch.optim as optim
from envs.combat_env import CombatEnv_8v8
from utils.geometry import get_distance, normalize
from agents.red_rl.policy import RedPolicy
from agents.red_rl.buffer import PPOBuffer

class RedTrainer:
    def __init__(self, config):
        self.config = config
        self.env = CombatEnv_8v8()
        self.policy = RedPolicy(config)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=config['lr'])
        self.device = self.policy.device
        
        # Buffer é…ç½®
        # éœ€è¦æ‰‹åŠ¨å®šä¹‰ Obs Shapeï¼Œè¿™å¿…é¡»ä¸ obs_parser ä¿æŒä¸€è‡´
        obs_shapes = {'self': 11, 'allies': (8, 7), 'enemies': (8, 7), 'missiles': (24, 7)}
        act_shapes = {} # å ä½
        self.buffer = PPOBuffer(config['steps_per_epoch'], 8, obs_shapes, act_shapes, self.device)
        
    def _map_actions_to_env(self, man_indices, tar_indices, obs_uids):
        """
        å°†ç¥ç»ç½‘ç»œè¾“å‡ºçš„ Index è½¬æ¢ä¸º Env éœ€è¦çš„ UID Dict
        å¿…é¡»é’ˆå¯¹æ¯ä¸ª Agent çš„è§†è§’ï¼Œå¯¹æ•ŒæœºæŒ‰è·ç¦»æ’åºï¼Œä»¥åŒ¹é… ObservationParser çš„é€»è¾‘ã€‚
        """
        env_actions = {}
        
        # 1. è·å–æ‰€æœ‰æ´»ç€çš„è“æ–¹æ•Œæœº (ä½œä¸ºåŸºç¡€åˆ—è¡¨)
        # æ³¨æ„ï¼šå¿…é¡»è¿‡æ»¤æ‰éæ´»è·ƒçš„ï¼Œå¦åˆ™è·ç¦»æ’åºä¼šåŒ…å«æ­»æ‰çš„é£æœºï¼Œå¯¼è‡´ç´¢å¼•åç§»ï¼Œä¸ ObsParser ä¸ä¸€è‡´
        blue_enemies = [p for p in self.env.sim.aircrafts if p.team == 1 and p.is_active]
        
        for i, uid in enumerate(obs_uids):
            maneuver_id = int(man_indices[i])
            target_idx = int(tar_indices[i])
            fire_target_uid = None
            
            # è·å–å½“å‰ Agent å¯¹è±¡
            agent = self.env.sim.get_entity(uid)
            
            if agent and agent.is_active:

                # 1. è®¡ç®—è¯¥ Agent åˆ°æ‰€æœ‰æ•Œæœºçš„è·ç¦»
                # è¿™ä¸€æ­¥æ˜¯æ ¸å¿ƒï¼šå¿…é¡»æ¨¡æ‹Ÿ Agent çœ‹åˆ°çš„â€œä¸–ç•Œâ€ï¼Œå³æŒ‰è·ç¦»è¿œè¿‘æ’åˆ—çš„æ•Œäºº
                sorted_enemies = []
                for enemy in blue_enemies:
                    dist = get_distance(agent.pos, enemy.pos) # ä½¿ç”¨ utils.geometry ä¸­çš„å‡½æ•°
                    sorted_enemies.append((dist, enemy))
                
                # 2. æŒ‰è·ç¦»ä»å°åˆ°å¤§æ’åº (Obs Index 0 = Nearest Enemy)
                sorted_enemies.sort(key=lambda x: x[0])
                
                # 3. æ ¹æ® Network è¾“å‡ºçš„ Index é€‰æ‹©ç›®æ ‡
                # target_idx æ˜¯ç½‘ç»œè¾“å‡ºçš„ 0-8 (8ä»£è¡¨ä¸å¼€ç«)
                # åªæœ‰å½“ target_idx æŒ‡å‘æœ‰æ•ˆçš„æ•Œæœºç´¢å¼•æ—¶æ‰å¼€ç«
                if target_idx < len(sorted_enemies):
                    # å–å‡ºæ’åºåçš„ç¬¬ target_idx ä¸ªæ•Œæœºå¯¹è±¡
                    target_obj = sorted_enemies[target_idx][1]
                    fire_target_uid = target_obj.uid
                # === [å…³é”®ä¿®å¤] ç»“æŸ ===
            
            env_actions[uid] = {
                'maneuver': maneuver_id,
                'fire_target': fire_target_uid
            }
            
        return env_actions

    def collect_rollouts(self):
        """
        é‡‡é›†æ•°æ®å¾ªç¯
        """
        print(f"\n{'='*20} Start New Rollout Episode {'='*20}")

        obs_dict = self.env.reset()
        red_uids = [f"Red_{i}" for i in range(8)]
        ep_ret = np.zeros(8)
        
        # ç»Ÿè®¡å˜é‡
        stat_speed = []
        stat_dist = []
        stat_fire = 0
        
        for t in range(self.config['steps_per_epoch']):
            # 1. æ•´ç† Batch Obs
            first_obs = list(obs_dict.values())[0]
            batch_obs = {
                k: np.zeros((8, *v.shape), dtype=np.float32) 
                for k, v in first_obs.items()
            }
            
            alive_mask = np.zeros(8, dtype=bool)
            for i, uid in enumerate(red_uids):
                if uid in obs_dict:
                    alive_mask[i] = True
                    for k in batch_obs:
                        batch_obs[k][i] = obs_dict[uid][k]
            
            # 2. ç¥ç»ç½‘ç»œæ¨ç†
            acts, logps, vals = self.policy.act(batch_obs)
            
            # 3. æ„é€  Env Action
            env_action_dict = self._map_actions_to_env(acts[0], acts[1], red_uids)
            
            # 4. Step
            next_obs_dict, rewards, dones, info = self.env.step(env_action_dict)

            # --- æ—¥å¿—æ‰“å° ---
            if 'events' in info:
                for event in info['events']:
                    if event['type'] == 'FIRE':
                        launcher = self.env.sim.get_entity(event['launcher'])
                        target = self.env.sim.get_entity(event['target'])
                        if launcher and target:
                            dist = get_distance(launcher.pos, target.pos)
                            vel_dir = normalize(launcher.vel)
                            los_dir = normalize(target.pos - launcher.pos)
                            angle = np.degrees(np.arccos(np.clip(np.dot(vel_dir, los_dir), -1, 1)))
                            print(f"[FIRE] ğŸš€ {launcher.uid} -> Locked {target.uid} | "
                                  f"Dist: {dist/1000:.1f}km | Angle: {angle:.1f}Â°")

                    elif event['type'] == 'KILL':
                        print(f"[KILL] ğŸ’¥ {event['killer']} HIT {event['victim']}!")

            if info['mean_speed'] > 1.0: 
                stat_speed.append(info['mean_speed'])
            stat_dist.append(info['mean_dist'])
            stat_fire += info['fire_count']
            
            # 5. æ•´ç† Reward
            rew_arr = np.zeros(8)
            for i, uid in enumerate(red_uids):
                rew_arr[i] = rewards.get(uid, 0.0)
            
            ep_ret += rew_arr
            
            # 6. å­˜å…¥ Buffer
            self.buffer.store(batch_obs, acts, rew_arr, vals, (logps[0], logps[1]))
            
            obs_dict = next_obs_dict
            
            # å¤„ç†å›åˆç»“æŸ
            timeout = (t == self.config['steps_per_epoch'] - 1)
            all_done = dones.get("__all__", False)
            
            if all_done or timeout:
                if timeout and not all_done:
                    last_val_obs = {
                        k: np.zeros((8, *v.shape), dtype=np.float32) 
                        for k, v in first_obs.items()
                    }
                    for i, uid in enumerate(red_uids):
                        if uid in obs_dict:
                            for k in last_val_obs:
                                last_val_obs[k][i] = obs_dict[uid][k]
                    _, _, last_vals = self.policy.act(last_val_obs)
                else:
                    last_vals = np.zeros(8)
                    
                self.buffer.finish_path(last_vals)
                
                # æ‰“å°ç»Ÿè®¡
                avg_speed = np.mean(stat_speed) if stat_speed else 0.0
                avg_dist_km = np.mean(stat_dist) / 1000.0 if stat_dist else 0.0
                red_left = sum(1 for p in self.env.sim.aircrafts if p.team==0 and p.is_active)
                blue_left = sum(1 for p in self.env.sim.aircrafts if p.team==1 and p.is_active)
                
                print(f"{'-'*10} Episode End {'-'*10}")
                print(f"[Result] Red Survivors: {red_left} | Blue Survivors: {blue_left}")
                print(f"  Ep Ret: {np.mean(ep_ret):.2f} | "
                      f"Spd: {avg_speed:.0f} m/s | "
                      f"Dist: {avg_dist_km:.1f} km | "
                      f"Fire: {stat_fire}")
                
                stat_speed = []
                stat_dist = []
                stat_fire = 0
                obs_dict = self.env.reset()
                ep_ret = np.zeros(8)

    def update(self):
        """
        PPO æ›´æ–°é€»è¾‘
        """
        data = self.buffer.get()
        
        clip_ratio = self.config['clip_ratio']
        target_kl = self.config['target_kl']
        entropy_coef = self.config['entropy_coef']
        train_iters = self.config['train_iters']
        
        loss_pi_list = []
        loss_v_list = []
        loss_ent_list = [] # [æ–°å¢] è®°å½• Entropy
        
        for i in range(train_iters):
            self.optimizer.zero_grad()
            
            # 1. é‡æ–°è¯„ä¼°æ—§æ ·æœ¬
            logps, ents, vals = self.policy.evaluate(
                data['obs'], 
                data['act_man'], 
                data['act_tar']
            )
            
            # 2. è®¡ç®— Ratio
            old_logp = data['logp_man'] + data['logp_tar']
            curr_logp = logps[0] + logps[1]
            ratio = torch.exp(curr_logp - old_logp)
            
            # 3. Policy Loss
            adv = data['adv']
            surr1 = ratio * adv
            surr2 = torch.clamp(ratio, 1 - clip_ratio, 1 + clip_ratio) * adv
            loss_pi = -(torch.min(surr1, surr2)).mean()
            
            # 4. Entropy Bonus
            total_entropy = ents[0] + ents[1]
            loss_ent = -total_entropy.mean() * entropy_coef
            
            # 5. Value Loss
            loss_v = ((vals - data['ret'])**2).mean()
            
            # Total Loss
            loss = loss_pi + loss_v + loss_ent
            
            # Kl Divergence check
            with torch.no_grad():
                approx_kl = (old_logp - curr_logp).mean().item()
            
            if approx_kl > 1.5 * target_kl:
                print(f"Early stopping at step {i} due to KL ({approx_kl:.4f})")
                break
                
            loss.backward()
            self.optimizer.step()
            
            loss_pi_list.append(loss_pi.item())
            loss_v_list.append(loss_v.item())
            loss_ent_list.append(total_entropy.mean().item()) # è®°å½•åŸå§‹ Entropy å€¼
            
        return np.mean(loss_pi_list), np.mean(loss_v_list), approx_kl, np.mean(loss_ent_list)

    def save_model(self, path):
        torch.save(self.policy.state_dict(), path)